
@inproceedings{dedic_balancing_2024,
	location = {Vienna, Austria},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://openreview.net/forum?id=DrHwIzz93C},
	abstract = {We present a method for graph node classification that allows a user to precisely select the resolution at which the graph in question should be simplified and through this provides a way of choosing a suitable point in the performance-complexity trade-off. The method is based on refining a reduced graph in a targeted way following the node classification confidence for particular nodes.},
	eventtitle = {The Twelfth International Conference on Learning Representations},
	booktitle = {The Second Tiny Papers Track at {ICLR} 2024},
	author = {Dědič, Marek and Bajer, Lukas and Prochazka, Pavel and Holena, Martin},
	urldate = {2024-04-18},
	date = {2024-05-11},
	langid = {english},
}

@inproceedings{prochazka_which_2023,
	location = {Tatranské Matliare, Slovakia},
	title = {Which Graph Properties Affect {GNN} Performance for a Given Downstream Task?},
	volume = {3498},
	rights = {All rights reserved},
	url = {https://ceur-ws.org/Vol-3498/#paper7},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Machine learning algorithms on graphs, in particular graph neural networks, became a popular framework for solving various tasks on graphs, attracting significant interest in the research community in recent years. As presented, however, these algorithms usually assume that the input graph is fixed and well-defined and do not consider the problem of constructing the graph for a given practical task. This work proposes a methodical way of linking graph properties with the performance of a {GNN} solving a given task on such graph via a surrogate regression model that is trained to predict the performance of the {GNN} from the properties of the graph dataset. Furthermore, the {GNN} model hyper-parameters are optionally added as additional features of the surrogate model and it is shown that this technique can be used to solve the practical problem of hyper-parameter tuning. We experimentally evaluate the importance of graph properties as features of the surrogate model with regards to the node classification task for several common graph datasets and discuss how these results can be used for graph composition tailored to the given task. Finally, our experiments indicate a significant gain in the proposed hyper-parameter tuning method compared to the reference grid-search method.},
	eventtitle = {Information Technologies – Applications and Theory 2023},
	pages = {58--66},
	booktitle = {Proceedings of the 23nd Conference Information Technologies – Applications and Theory ({ITAT} 2023)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2023-09-30},
	date = {2023-10-07},
	langid = {english},
}

@misc{pevny_nested_2020,
	title = {Nested Multiple Instance Learning in Modelling of {HTTP} network traffic},
	rights = {All rights reserved},
	url = {http://arxiv.org/abs/2002.04059},
	doi = {10.48550/arXiv.2002.04059},
	abstract = {In many interesting cases, the application of machine learning is hindered by data having a complicated structure stimulated by a structured file-formats like {JSONs}, {XMLs}, or {ProtoBuffers}, which is non-trivial to convert to a vector / matrix. Moreover, since the structure frequently carries a semantic meaning, reflecting it in the machine learning model should improve the accuracy but more importantly it facilitates the explanation of decisions and the model. This paper demonstrates on the identification of infected computers in the computer network from their {HTTP} traffic, how to achieve this reflection using recent progress in multiple-instance learning. The proposed model is compared to complementary approaches from the prior art, the first relying on human-designed features and the second on automatically learned features through convolution neural networks. In a challenging scenario measuring accuracy only on unseen domains/malware families, the proposed model is superior to the prior art while providing a valuable feedback to the security researchers. We believe that the proposed framework will found applications elsewhere even beyond the field of security.},
	number = {{arXiv}:2002.04059},
	publisher = {{arXiv}},
	author = {Pevny, Tomas and Dedic, Marek},
	urldate = {2023-10-11},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {2002.04059 [cs]},
}

@inproceedings{dedic_balancing_2023,
	location = {Torino, Italy},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://mlg-europe.github.io/2023/papers/234.pdf},
	abstract = {Abstract Graph based models are used for tasks with increasing size and computational demands. We present a method for node classification that allows a user to precisely select the resolution at which the graph in question should be pretrained. Our method builds on an existing algorithm for pretraining on coarser graphs, {HARP}, which we extend in order to tune the effect of graph coarsening on the accuracy of node classification on a fine level. We present a novel way of refining the reduced graph in a targeted way based on the node classification confidence of particular nodes. This enhancement provides sufficient detail where needed, while collapsing structures where per-node information is not necessary for sufficient node classification accuracy. Hence, the method provides a meta-model for enhancing graph embedding models such as node2vec. We apply it to several datasets and discuss the differing behaviour on each of them in the context of their properties.},
	eventtitle = {20th International Workshop on Mining and Learning with Graphs},
	booktitle = {20th International Workshop on Mining and Learning with Graphs @{ECMLPKDD} 2023},
	author = {Dědič, Marek and Bajer, Lukáš and Procházka, Pavel and Holeňa, Martin},
	urldate = {2023-09-22},
	date = {2023-09-22},
	langid = {english},
}

@inproceedings{dedic_loss_2020,
	location = {Oravská Lesná, Slovakia},
	title = {Loss Functions for Clustering in Multi-instance Learning},
	volume = {2718},
	url = {http://ceur-ws.org/Vol-2718/#paper05},
	abstract = {Multi-instance learning belongs to one of recently fast developing areas of machine learning. It is a supervised learning method and this paper reports research into its unsupervised counterpart, multi-instance clustering. Whereas traditional clustering clusters points, multi-instance clustering clusters bags, i.e. multisets of points or of other kinds of objects. The paper focuses on the problem of loss functions for clustering. Three sophisticated loss functions used for clustering of points, contrastive predictive coding, triplet loss and magnet loss, are elaborated for multi-instance clustering. Finally, they are compared on 18 benchmark datasets, as well as on a real-world dataset.},
	eventtitle = {Information Technologies - Applications and Theory ({ITAT} 2020)},
	pages = {137--146},
	booktitle = {Proceedings of the 20th Conference Information Technologies - Applications and Theory ({ITAT} 2020)},
	publisher = {{CEUR}-{WS}.org},
	author = {Dědič, Marek and Pevný, Tomáš and Bajer, Lukáš and Holeňa, Martin},
	date = {2020-10-30},
	file = {Full Text:/home/personal/Zotero/storage/UV9IPN8W/Dědič et al. - Loss Functions for Clustering in Multi-instance Le.pdf:application/pdf},
}

@article{yuan_explainability_2022,
	title = {Explainability in graph neural networks: A taxonomic survey},
	volume = {45},
	shorttitle = {Explainability in graph neural networks},
	pages = {5782--5799},
	number = {5},
	journaltitle = {{IEEE} transactions on pattern analysis and machine intelligence},
	author = {Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
	date = {2022},
	note = {Publisher: {IEEE}},
}

@inproceedings{ying_gnnexplainer_2019,
	title = {{GNNExplainer}: Generating Explanations for Graph Neural Networks},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html},
	shorttitle = {{GNNExplainer}},
	abstract = {Graph Neural Networks ({GNNs}) are a powerful tool for machine learning on graphs.{GNNs} combine node feature information with the graph structure by 
recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models,
and explaining predictions made by {GNNs} remains unsolved. Here
we propose {GNNExplainer}, the first general, model-agnostic approach for providing interpretable explanations for predictions of any {GNN}-based model on any graph-based machine learning task. Given an instance, {GNNExplainer} identifies a compact subgraph structure and a small subset of node features that have a crucial role in {GNN}'s prediction. 
Further, {GNNExplainer}  can generate consistent and concise explanations for an entire class of instances.
We formulate {GNNExplainer} as an optimization task that maximizes the mutual information between a {GNN}'s prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1\% on average. {GNNExplainer}  provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty {GNNs}.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
	urldate = {2023-08-30},
	date = {2019},
}

@report{shapley_notes_1951,
	title = {Notes on the N-Person Game — {II}: The Value of an N-Person Game},
	url = {https://www.rand.org/pubs/research_memoranda/RM0670.html},
	shorttitle = {Notes on the N-Person Game — {II}},
	institution = {{RAND} Corporation},
	author = {Shapley, Lloyd S.},
	urldate = {2023-06-29},
	date = {1951-08-21},
	langid = {english},
	keywords = {Game Theory},
}

@inproceedings{benavoli_bayesian_2014,
	location = {Beijing, China},
	title = {A Bayesian Wilcoxon signed-rank test based on the Dirichlet process},
	url = {\url{https://proceedings.mlr.press/v32/benavoli14.html}},
	abstract = {Bayesian methods are ubiquitous in machine learning.  Nevertheless, the analysis of empirical results is typically   performed  by frequentist tests. This implies dealing with  null hypothesis significance tests and  p-values, even though the   shortcomings of such methods are well known.   We propose  a nonparametric Bayesian version of the Wilcoxon   signed-rank test using a Dirichlet process ({DP}) based prior.  We address in two different ways the problem of how to choose  the   infinite dimensional parameter that characterizes the {DP}.   The proposed  test has all the traditional strengths of the Bayesian   approach; for instance, unlike the frequentist tests,   it allows verifying the null hypothesis, not only rejecting it, and   taking decision which minimize the expected loss.  Moreover, one of the solutions proposed to model the infinitedimensional parameter of the {DP}, allows isolating instances in which the traditional frequentist test is guessing at random.   We show results dealing with the comparison of two classifiers using real and simulated data.},
	eventtitle = {The 31st International Conference on Machine Learning},
	pages = {1026--1034},
	booktitle = {Proceedings of the 31st International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca and Zaffalon, Marco and Ruggeri, Fabrizio},
	urldate = {2022-12-07},
	date = {2014-06-18},
	langid = {english},
	note = {{ISSN}: 1938-7228},
}

@inproceedings{chiang_cluster-gcn_2019,
	location = {New York, {NY}, {USA}},
	title = {Cluster-{GCN}: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks},
	isbn = {978-1-4503-6201-6},
	url = {https://doi.org/10.1145/3292500.3330925},
	doi = {10.1145/3292500.3330925},
	series = {{KDD} '19},
	shorttitle = {Cluster-{GCN}},
	abstract = {Graph convolutional network ({GCN}) has been successfully applied to many graph-based applications; however, training a large-scale {GCN} remains challenging. Current {SGD}-based algorithms suffer from either a high computational cost that exponentially grows with number of {GCN} layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-{GCN}, a novel {GCN} algorithm that is suitable for {SGD}-based training by exploiting the graph clustering structure. Cluster-{GCN} works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer {GCN} on this data, Cluster-{GCN} is faster than the previous state-of-the-art {VR}-{GCN} (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer {GCN} on this data, our algorithm can finish in around 36 minutes while all the existing {GCN} training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-{GCN} allows us to train much deeper {GCN} without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-{GCN}, we achieve state-of-the-art test F1 score 99.36 on the {PPI} dataset, while the previous best result was 98.71 by{\textasciitilde}{\textbackslash}citezhang2018gaan.},
	pages = {257--266},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
	urldate = {2022-11-28},
	date = {2019-07-25},
	keywords = {graph convolutional networks, clustering, deep learning, large-scale learning, semi-supervised learning},
}

@inproceedings{dedic_adaptive_2022,
	location = {Grenoble, France},
	title = {Adaptive graph coarsening in the context of local graph quality},
	rights = {All rights reserved},
	url = {https://graphquality.github.io/rsc/art1.pdf},
	abstract = {Graph based models are used for tasks with increasing size and computational demands. We present a method for studying graph properties from the point of view of a downstream task. More precisely, the method allows a user to precisely select the resolution at which the graph in question should be coarsened. Our method builds on an existing algorithm for pretraining on coarser graphs, {HARP}. We extend both main parts of the algorithm in order to observe the effect of graph coarsening to model quality on a fine level. We present a general framework for graph coarsenings, providing two alternative algorithms based on graph diffusion convolution and evolutionary algorithms. Additionally, we present a novel way for un-coarsening the reduced graph in a targeted way based on the confidence of downstream classification for particular nodes. Together, these enhancements provide sufficient detail where needed, while collapsing structures where per-node information is not necessary for high model performance. Our method is a general meta-model for enhancing graph embedding models such as node2vec. We apply the method to several datasets and discuss the differing behaviour on each of them. Furthermore, we compare the proposed coarsening schemas.},
	eventtitle = {Data and Model Quality for Mining and Learning with Graphs: Methods and Open Challenges @{ECML}-{PKDD} 2022},
	booktitle = {Data and Model Quality for Mining and Learning with Graphs: Methods and Open Challenges @{ECML}-{PKDD} 2022},
	author = {Dědič, Marek and Bajer, Lukáš and Repický, Jakub and Procházka, Pavel and Holeňa, Martin},
	urldate = {2022-09-23},
	date = {2022-09-23},
	langid = {english},
}

@inproceedings{prochazka_scalable_2022,
	location = {Zuberec, Slovakia},
	title = {Scalable Graph Size Reduction for Efficient {GNN} Application},
	volume = {3226},
	rights = {All rights reserved},
	url = {http://ceur-ws.org/Vol-3226/#paper9},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Graph neural networks ({GNN}) present a dominant framework for representation learning on graphs for the past several years. The main strength of {GNNs} lies in the fact that they can simultaneously learn from both node related attributes and relations between nodes, represented by edges. In tasks leading to large graphs, {GNN} often requires significant computational resources to achieve its superior performance. In order to reduce the computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple scalable task-aware graph preprocessing procedure allowing us to obtain a reduced graph such as {GNN} achieves a given desired performance on the downstream task. In addition, the proposed preprocessing allows for fitting the reduced graph and {GNN} into a given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional {GNN} benchmark datasets.},
	eventtitle = {Information Technologies – Applications and Theory 2022},
	pages = {75--84},
	booktitle = {Proceedings of the 22nd Conference Information Technologies – Applications and Theory ({ITAT} 2022)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2022-09-30},
	date = {2022-09-30},
	langid = {english},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: Scalable feature learning for networks},
	shorttitle = {node2vec},
	pages = {855--864},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	date = {2016},
	file = {Full Text:/home/personal/Zotero/storage/Y8U8YZQI/PMC5108654.html:text/html;Snapshot:/home/personal/Zotero/storage/ITX5DJ45/2939672.html:text/html},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: Online learning of social representations},
	shorttitle = {Deepwalk},
	pages = {701--710},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	date = {2014},
	file = {Full Text:/home/personal/Zotero/storage/ISN68YG2/Perozzi et al. - 2014 - Deepwalk Online learning of social representation.pdf:application/pdf;Snapshot:/home/personal/Zotero/storage/Y3TWLT7L/2623330.html:text/html},
}

@inproceedings{kipf_semi-supervised_2017,
	location = {Toulon, France},
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	url = {https://openreview.net/forum?id=SJU4ayYgl},
	abstract = {Semi-supervised classification with a {CNN} model for graphs. State-of-the-art results on a number of citation network datasets.},
	eventtitle = {International Conference on Learning Representations ({ICLR})},
	booktitle = {5th International Conference on Learning Representations, \{{ICLR}\} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher = {{OpenReview}.net},
	author = {Kipf, Thomas N. and Welling, Max},
	date = {2017-04-24},
	langid = {english},
}

@inproceedings{borisov_experimental_2021,
	location = {Heľpa, Slovakia},
	title = {Experimental Investigation of Neural and Weisfeiler-Lehman-Kernel Graph Representations for Downstream Classification},
	volume = {2962},
	rights = {All rights reserved},
	url = {http://ceur-ws.org/Vol-2962/#paper50},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Graphs are one of the most ubiquitous kinds of data. However, data analysis methods have been developed primarily for numerical data, and to make use of them, graphs need to be represented as elements of some Euclidean space. An increasingly popular way of representing them in this way are graph neural networks ({GNNs}). Because data analysis applications typically require identical results for isomorphic graphs, the representations learned by {GNNs} also need to be invariant with respect to graph isomorphism. That motivated recent research into the possibilities of recognizing nonisomorphic pairs of graphs by {GNNs}, primarily based on the Weisfeiler-Lehman ({WL}) isomorphism test. This paper reports the results of a first experimental comparison of four variants of two important {GNNs} based on the {WL} test from the point of view of graph representation for downstream classification by means of a support vector machins ({SVM}). Those methods are compared not only with each other, but also with a recent generalization of the {WL} subtree kernel. For all {GNN} variants, two different representations are included in the comparison. The comparison revealed that the four considered representations of the same kind of {GNN} never significantly differ. On the other hand, there was always a statistically significant difference between representations originating from different kinds of {GNNs}, as well as between any representation originating from any of the considered {GNNs} and the representation originating from the generalized {WL} kernel.},
	eventtitle = {Information Technologies – Applications and Theory 2021},
	pages = {130--139},
	booktitle = {Proceedings of the 21st Conference Information Technologies – Applications and Theory ({ITAT} 2021)},
	publisher = {{CEUR}-{WS}.org},
	author = {Borisov, Sergej and Dědič, Marek and Holeňa, Martin},
	urldate = {2021-10-07},
	date = {2021-10-02},
	langid = {english},
	note = {{ISSN}: 1613-0073},
	file = {Full Text PDF:/home/personal/Zotero/storage/Z5KJKGR4/Borisov et al. - 2021 - Experimental Investigation of Neural and Weisfeile.pdf:application/pdf},
}

@article{chen_harp_2018,
	title = {{HARP}: Hierarchical Representation Learning for Networks},
	volume = {32},
	rights = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11849},
	shorttitle = {{HARP}},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Chen, Haochen and Perozzi, Bryan and Hu, Yifan and Skiena, Steven},
	urldate = {2021-05-25},
	date = {2018-04-26},
	langid = {english},
	note = {Number: 1},
	keywords = {network classification},
}

@inproceedings{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	pages = {1024--1034},
	booktitle = {Advances in neural information processing systems},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	date = {2017},
	file = {Full Text:/home/personal/Zotero/storage/5Y9Q2N8U/Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf;Snapshot:/home/personal/Zotero/storage/B8EPL4ZN/6703-inductive-representation-learning-on-large-graphs.html:text/html},
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {https://openreview.net/forum?id=idpCdOWtqXd60},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity...},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2020-11-03},
	date = {2013-01-17},
	langid = {english},
	file = {Snapshot:/home/personal/Zotero/storage/KERQU498/forum.html:text/html},
}

@online{kubara_machine_2020,
	title = {Machine Learning Tasks on Graphs},
	url = {https://towardsdatascience.com/machine-learning-tasks-on-graphs-7bc8f175119a},
	abstract = {Can We Divide It Into Supervised/Unsupervised Learning? It’s Not That Simple…},
	titleaddon = {Medium},
	author = {Kubara, Kacper},
	urldate = {2020-11-03},
	date = {2020-09-28},
	langid = {english},
	file = {Snapshot:/home/personal/Zotero/storage/4IYMYT34/machine-learning-tasks-on-graphs-7bc8f175119a.html:text/html},
}

@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network ({GNN}), capable of directly processing graphs. {GNNs} extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for {GNNs} is proposed and some experiments are discussed which assess the properties of the model.},
	eventtitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	pages = {729--734 vol. 2},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	date = {2005-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Neural networks, Machine learning, Machine learning algorithms, Application software, data structures, Data structures, Encoding, Focusing, graph neural network, graph theory, graphical data structures, learning (artificial intelligence), learning algorithm, neural nets, Recurrent neural networks, recursive neural networks, Software engineering, Tree graphs},
	file = {IEEE Xplore Abstract Record:/home/personal/Zotero/storage/33929VKF/1555942.html:text/html},
}

@article{dietterich_solving_1997,
	title = {Solving the multiple instance problem with axis-parallel rectangles},
	volume = {89},
	issn = {0004-3702},
	doi = {10.1016/S0004-3702(96)00034-3},
	abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
	pages = {31--71},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
	urldate = {2017-05-31},
	date = {1997-01-01},
	keywords = {Machine learning, Drug design, Structure-activity relationships},
}

@unpublished{zhu_learning_2003,
	title = {Learning from Labeled and Unlabeled Data with Label Propagation},
	abstract = {We investigate the use of unlabeled data to help labeled data in classification.},
	type = {Technical Report},
	howpublished = {Technical Report},
	author = {Zhu, Xiaojin and Ghahramani, Zoubin},
	date = {2003-07-11},
}

@article{bai_hypergraph_2021,
	title = {Hypergraph convolution and hypergraph attention},
	volume = {110},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320304404},
	doi = {10.1016/j.patcog.2020.107637},
	abstract = {Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.},
	pages = {107637},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Bai, Song and Zhang, Feihu and Torr, Philip H. S.},
	urldate = {2024-06-12},
	date = {2021-02-01},
	keywords = {Graph learning, Graph neural networks, Hypergraph learning, Semi-supervised learning},
	file = {ScienceDirect Snapshot:/home/personal/Zotero/storage/ZQLWYURM/S0031320320304404.html:text/html;Submitted Version:/home/personal/Zotero/storage/28QPHHZY/Bai et al. - 2021 - Hypergraph convolution and hypergraph attention.pdf:application/pdf},
}

@book{levi_finite_1942,
	location = {Calcutta},
	title = {Finite geometrical systems},
	publisher = {University of Calcutta},
	author = {Levi, Friedrich Wilhelm},
	date = {1942},
}

@inproceedings{prochazka_convolutional_2024,
	location = {Vilnius, Lithuania},
	title = {Convolutional Signal Propagation: A Simple Scalable Algorithm for Hypergraphs},
	rights = {All rights reserved},
	url = {https://mlg-europe.github.io/2024/papers/131/CameraReady/MLG-ECML-2024-paper.pdf},
	abstract = {Last decade has seen the emergence of numerous methods for learning on graphs, particularly Graph Neural Networks ({GNNs}). These methods, however, are often not directly applicable to more complex structures like bipartite graphs (equivalent to hypergraphs), which represent interactions among two entity types (e.g. a user liking a movie). This paper proposes Convolutional Signal Propagation ({CSP}), a non-parametric simple and scalable method that natively operates on bipartite graphs (hypergraphs) and can be implemented with just a few lines of code. After defining {CSP}, we demonstrate its relationship with well-established methods like label propagation, Naive Bayes, and Hypergraph Convolutional Networks. We evaluate {CSP} against several reference methods on real-world datasets from multiple domains, focusing on retrieval and classification tasks. Our results show that {CSP} offers competitive performance while maintaining low computational complexity, making it an ideal first choice as a baseline for hypergraph node classification and retrieval. Moreover, despite operating on hypergraphs, {CSP} achieves good results in tasks typically not associated with hypergraphs, such as natural language processing.},
	eventtitle = {21st International Workshop on Mining and Learning with Graphs},
	booktitle = {21st International Workshop on Mining and Learning with Graphs @{ECMLPKDD} 2024},
	author = {Procházka, Pavel and Dědič, Marek and Bajer, Lukáš},
	date = {2024-09-09},
	langid = {english},
}

@article{akoglu_graph_2015,
	title = {Graph based anomaly detection and description: a survey},
	volume = {29},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-014-0365-y},
	doi = {10.1007/s10618-014-0365-y},
	shorttitle = {Graph based anomaly detection and description},
	abstract = {Detecting anomalies in data is a vital task, with numerous high-impact applications in areas such as security, finance, health care, and law enforcement. While numerous techniques have been developed in past years for spotting outliers and anomalies in unstructured collections of multi-dimensional points, with graph data becoming ubiquitous, techniques for structured graph data have been of focus recently. As objects in graphs have long-range correlations, a suite of novel technology has been developed for anomaly detection in graph data. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for anomaly detection in data represented as graphs. As a key contribution, we give a general framework for the algorithms categorized under various settings: unsupervised versus (semi-)supervised approaches, for static versus dynamic graphs, for attributed versus plain graphs. We highlight the effectiveness, scalability, generality, and robustness aspects of the methods. What is more, we stress the importance of anomaly attribution and highlight the major techniques that facilitate digging out the root cause, or the ‘why’, of the detected anomalies for further analysis and sense-making. Finally, we present several real-world applications of graph-based anomaly detection in diverse domains, including financial, auction, computer traffic, and social networks. We conclude our survey with a discussion on open theoretical and practical challenges in the field.},
	pages = {626--688},
	number = {3},
	journaltitle = {Data Mining and Knowledge Discovery},
	shortjournal = {Data Min Knowl Disc},
	author = {Akoglu, Leman and Tong, Hanghang and Koutra, Danai},
	urldate = {2024-08-29},
	date = {2015-05-01},
	langid = {english},
	keywords = {Artificial Intelligence, Anomaly description, Anomaly detection, Change point detection, Event detection, Fraud detection, Graph mining, Network anomaly detection, Visual analytics},
	file = {Submitted Version:/home/personal/Zotero/storage/PC5CRT85/Akoglu et al. - 2015 - Graph based anomaly detection and description a survey.pdf:application/pdf},
}

@misc{berg_graph_2017,
	title = {Graph Convolutional Matrix Completion},
	url = {http://arxiv.org/abs/1706.02263},
	doi = {10.48550/arXiv.1706.02263},
	abstract = {We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.},
	number = {{arXiv}:1706.02263},
	publisher = {{arXiv}},
	author = {Berg, Rianne van den and Kipf, Thomas N. and Welling, Max},
	urldate = {2024-08-29},
	date = {2017-10-25},
	eprinttype = {arxiv},
	eprint = {1706.02263 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Databases, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/home/personal/Zotero/storage/89V6T694/Berg et al. - 2017 - Graph Convolutional Matrix Completion.pdf:application/pdf;arXiv.org Snapshot:/home/personal/Zotero/storage/MNKKADPY/1706.html:text/html},
}

@inproceedings{zhang_every_2020,
	location = {Online},
	title = {Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks},
	url = {https://aclanthology.org/2020.acl-main.31},
	doi = {10.18653/v1/2020.acl-main.31},
	shorttitle = {Every Document Owns Its Structure},
	abstract = {Text classification is fundamental in natural language processing ({NLP}) and Graph Neural Networks ({GNN}) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose {TextING} for inductive text classification via {GNN}. We first build individual graphs for each document and then use {GNN} to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.},
	eventtitle = {{ACL} 2020},
	pages = {334--339},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yufeng and Yu, Xueli and Cui, Zeyu and Wu, Shu and Wen, Zhongzhen and Wang, Liang},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2024-08-29},
	date = {2020-07},
	file = {Full Text PDF:/home/personal/Zotero/storage/M88BUQJP/Zhang et al. - 2020 - Every Document Owns Its Structure Inductive Text Classification via Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{duvenaud_convolutional_2015,
	title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
	abstract = {We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
	urldate = {2024-08-29},
	date = {2015},
	file = {Full Text PDF:/home/personal/Zotero/storage/I2CJPF9A/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Molecular Fingerprints.pdf:application/pdf},
}

@inproceedings{bordes_translating_2013,
	title = {Translating Embeddings for Modeling Multi-relational Data},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
	abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, {TransE}, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that {TransE} significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	urldate = {2024-08-29},
	date = {2013},
	file = {Full Text PDF:/home/personal/Zotero/storage/9IRXQUF3/Bordes et al. - 2013 - Translating Embeddings for Modeling Multi-relational Data.pdf:application/pdf},
}

@article{zhang_graph_2021,
	title = {Graph Neural Networks and Their Current Applications in Bioinformatics},
	volume = {12},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2021.690049/full},
	doi = {10.3389/fgene.2021.690049},
	abstract = {{\textless}p{\textgreater}Graph neural networks ({GNNs}), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, {GNNs} have also become an important tool in bioinformatics. In this research, a systematic survey of {GNNs} and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used {GNN} models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by {GNNs}: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although {GNNs} have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that {GNNs} are potentially an excellent method that solves various biological problems in bioinformatics research.{\textless}/p{\textgreater}},
	journaltitle = {Frontiers in Genetics},
	shortjournal = {Front. Genet.},
	author = {Zhang, Xiao-Meng and Liang, Li and Liu, Lin and Tang, Ming-Jing},
	urldate = {2024-08-29},
	date = {2021-07-29},
	note = {Publisher: Frontiers},
	keywords = {deep learning, Graph neural networks, bioinformatics, network biology, {OMICS} data},
	file = {Full Text:/home/personal/Zotero/storage/57UL5W2I/Zhang et al. - 2021 - Graph Neural Networks and Their Current Applications in Bioinformatics.pdf:application/pdf},
}

@article{wang_link_2015,
	title = {Link prediction in social networks: the state-of-the-art},
	volume = {58},
	issn = {1869-1919},
	url = {https://doi.org/10.1007/s11432-014-5237-y},
	doi = {10.1007/s11432-014-5237-y},
	shorttitle = {Link prediction in social networks},
	abstract = {In social networks, link prediction predicts missing links in current networks and new or dissolution links in future networks, is important for mining and analyzing the evolution of social networks. In the past decade, many works have been done about the link prediction in social networks. The goal of this paper is to comprehensively review, analyze and discuss the state-of-the-art of the link prediction in social networks. A systematical category for link prediction techniques and problems is presented. Then link prediction techniques and problems are analyzed and discussed. Typical applications of link prediction are also addressed. Achievements and roadmaps of some active research groups are introduced. Finally, some future challenges of the link prediction in social networks are discussed.},
	pages = {1--38},
	number = {1},
	journaltitle = {Science China Information Sciences},
	shortjournal = {Sci. China Inf. Sci.},
	author = {Wang, Peng and Xu, {BaoWen} and Wu, {YuRong} and Zhou, {XiaoYu}},
	urldate = {2024-08-29},
	date = {2015-01-01},
	langid = {english},
	keywords = {Artificial Intelligence, 011101, dynamic network, learning model, link prediction, similarity metric, social network, 动态网络, 学习模型, 相似度量, 社交网络, 链接预测},
	file = {Submitted Version:/home/personal/Zotero/storage/KUUABW74/Wang et al. - 2015 - Link prediction in social networks the state-of-the-art.pdf:application/pdf},
}

@article{maleki_learning_2023,
	title = {Learning to Explain Hypergraph Neural Networks},
	url = {https://openreview.net/forum?id=B6YeDatcFw},
	abstract = {Hypergraphs are expressive structures for describing higher-order relationships among entities, with widespread applications across biology and drug discovery. Hypergraph neural networks ({HGNNs}) have recently emerged as a promising representation learning approach on these structures for clustering, classification, and more. However, despite their promising performance, {HGNNs} remain a black box, and explaining how they make predictions remains an open challenge. To address this problem, we propose {HyperEX}, a post-hoc explainability framework for hypergraphs that can be applied to any trained {HGNN}. {HyperEX} computes node-hyperedge pair importance to identify sub-hypergraphs as explanations. Our experiments demonstrate how {HyperEX} learns important sub-hypergraphs responsible for driving node classification to give useful insight into {HGNNs}.},
	author = {Maleki, Sepideh and Hajiramezanali, Ehsan and Scalia, Gabriele and Biancalani, Tommaso and Chuang, Kangway V.},
	urldate = {2024-08-29},
	date = {2023-06-18},
	langid = {english},
	file = {Full Text PDF:/home/personal/Zotero/storage/WD6KLCF4/Maleki et al. - 2023 - Learning to Explain Hypergraph Neural Networks.pdf:application/pdf},
}

@article{zhou_rich-club_2004,
	title = {The rich-club phenomenon in the Internet topology},
	volume = {8},
	issn = {1558-2558},
	url = {https://ieeexplore.ieee.org/document/1278314},
	doi = {10.1109/LCOMM.2004.823426},
	abstract = {We show that the Internet topology at the autonomous system ({AS}) level has a rich-club phenomenon. The rich nodes, which are a small number of nodes with large numbers of links, are very well connected to each other. The rich-club is a core tier that we measured using the rich-club connectivity and the node-node link distribution. We obtained this core tier without any heuristic assumption between the {ASs}. The rich-club phenomenon is a simple qualitative way to differentiate between power law topologies and provides a criterion for new network models. To show this, we compared the measured rich-club of the {AS} graph with networks obtained using the Baraba/spl acute/si-Albert ({BA}) scale-free network model, the Fitness {BA} model and the Inet-3.0 model.},
	pages = {180--182},
	number = {3},
	journaltitle = {{IEEE} Communications Letters},
	author = {Zhou, Shi and Mondragon, R.J.},
	urldate = {2024-08-28},
	date = {2004-03},
	note = {Conference Name: {IEEE} Communications Letters},
	keywords = {Character generation, Character recognition, Councils, Internet, {IP} networks, Network topology, Power generation, Routing, Tail},
	file = {Full Text:/home/personal/Zotero/storage/VA3QEI4A/Zhou and Mondragon - 2004 - The rich-club phenomenon in the Internet topology.pdf:application/pdf;IEEE Xplore Abstract Record:/home/personal/Zotero/storage/IFUPTQAP/1278314.html:text/html},
}

@article{gilbert_random_1959,
	title = {Random Graphs},
	volume = {30},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full},
	doi = {10.1214/aoms/1177706098},
	abstract = {The Annals of Mathematical Statistics},
	pages = {1141--1144},
	number = {4},
	journaltitle = {The Annals of Mathematical Statistics},
	author = {Gilbert, E. N.},
	urldate = {2024-08-27},
	date = {1959-12},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Full Text PDF:/home/personal/Zotero/storage/BNF2GA6K/Gilbert - 1959 - Random Graphs.pdf:application/pdf},
}

@inproceedings{luo_parameterized_2020,
	title = {Parameterized Explainer for Graph Neural Network},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e37b08dd3015330dcbb5d6663667b8b8-Abstract.html},
	abstract = {Despite recent progress in Graph Neural Networks ({GNNs}), explaining predictions made by {GNNs} remains a challenging open problem.
The leading method mainly addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a {GNN} model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance
independently is not sufficient to provide a global understanding of the learned {GNN} model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class).
In this study, we address these key challenges and propose {PGExplainer}, a parameterized explainer for {GNNs}. {PGExplainer} adopts a deep neural network to parameterize the generation process of explanations, which enables {PGExplainer} a natural approach to multi-instance explanations. Compared to the existing work, {PGExplainer} has a better generalization power and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7{\textbackslash}\% relative improvement in {AUC} on explaining graph classification over the leading baseline.},
	pages = {19620--19631},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Luo, Dongsheng and Cheng, Wei and Xu, Dongkuan and Yu, Wenchao and Zong, Bo and Chen, Haifeng and Zhang, Xiang},
	urldate = {2024-08-27},
	date = {2020},
	file = {Full Text PDF:/home/personal/Zotero/storage/C4EJ6GJ4/Luo et al. - 2020 - Parameterized Explainer for Graph Neural Network.pdf:application/pdf},
}

@article{yamada_high-dimensional_2014,
	title = {High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso},
	volume = {26},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00537},
	doi = {10.1162/NECO_a_00537},
	abstract = {The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this letter, we consider a feature-wise kernelized Lasso for capturing nonlinear input-output dependency. We first show that with particular choices of kernel functions, nonredundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures such as the Hilbert-Schmidt independence criterion. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments for classification and regression with thousands of features.},
	pages = {185--207},
	number = {1},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Yamada, Makoto and Jitkrittum, Wittawat and Sigal, Leonid and Xing, Eric P. and Sugiyama, Masashi},
	urldate = {2024-08-26},
	date = {2014-01-01},
	file = {Snapshot:/home/personal/Zotero/storage/TW3RMD6Q/High-Dimensional-Feature-Selection-by-Feature-Wise.html:text/html;Submitted Version:/home/personal/Zotero/storage/UBVDAB6D/Yamada et al. - 2014 - High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso.pdf:application/pdf},
}

@inproceedings{ribeiro_why_2016,
	location = {San Francisco California {USA}},
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	shorttitle = {"Why Should I Trust You?},
	eventtitle = {{KDD} '16: The 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {1135--1144},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2024-08-26},
	date = {2016-08-13},
	langid = {english},
}

@article{huang_graphlime_2023,
	title = {{GraphLIME}: Local Interpretable Model Explanations for Graph Neural Networks},
	volume = {35},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9811416},
	doi = {10.1109/TKDE.2022.3187455},
	shorttitle = {{GraphLIME}},
	abstract = {Recently, graph neural networks ({GNN}) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. However, explaining the effectiveness of {GNN} models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose {GraphLIME}, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion ({HSIC}) Lasso, which is a nonlinear feature selection method. {GraphLIME} is a generic {GNN}-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. Through experiments on two real-world datasets, the explanations of {GraphLIME} are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.},
	pages = {6968--6972},
	number = {7},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Huang, Qiang and Yamada, Makoto and Tian, Yuan and Singh, Dinesh and Chang, Yi},
	urldate = {2024-08-26},
	date = {2023-07},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Computational modeling, Feature extraction, Kernel, Graph neural networks, Data models, explanation, interpretability, Mathematical models, Predictive models, Toy manufacturing industry},
	file = {IEEE Xplore Abstract Record:/home/personal/Zotero/storage/9YUZMA5I/9811416.html:text/html;Submitted Version:/home/personal/Zotero/storage/W5HC4JHC/Huang et al. - 2023 - GraphLIME Local Interpretable Model Explanations for Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: Visual Explanations From Deep Networks via Gradient-Based Localization},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
	shorttitle = {Grad-{CAM}},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
	pages = {618--626},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2024-08-26},
	date = {2017},
	file = {Full Text PDF:/home/personal/Zotero/storage/LZ2R4K73/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations From Deep Networks via Gradient-Based Localization.pdf:application/pdf},
}

@inproceedings{gilmer_neural_2017,
	location = {Sydney, {NSW}, Australia},
	title = {Neural message passing for Quantum chemistry},
	series = {{ICML}'17},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	pages = {1263--1272},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	publisher = {{JMLR}.org},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2024-08-26},
	date = {2017-08-06},
	file = {Full Text PDF:/home/personal/Zotero/storage/KM28LB45/Gilmer et al. - 2017 - Neural message passing for Quantum chemistry.pdf:application/pdf},
}

@book{vapnik_nature_1995,
	location = {New York, {NY}},
	title = {The Nature of Statistical Learning Theory},
	rights = {http://www.springer.com/tdm},
	isbn = {978-1-4757-2442-4 978-1-4757-2440-0},
	url = {http://link.springer.com/10.1007/978-1-4757-2440-0},
	publisher = {Springer},
	author = {Vapnik, Vladimir N.},
	urldate = {2024-08-24},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-1-4757-2440-0},
	keywords = {learning algorithm, algorithms, boundary element method, construction, controlling, convergence, function, functional, learning, learning theory, model, proof, Statistica, statistical theory, statistics, cognition, Conditional probability, control, pattern recognition, Statistical Learning, Statistical Theory},
	file = {Full Text:/home/personal/Zotero/storage/D8HX6ATF/Vapnik - 1995 - The Nature of Statistical Learning Theory.pdf:application/pdf;Full Text:/home/personal/Zotero/storage/U8AWMV69/Vapnik - 2000 - The Nature of Statistical Learning Theory.pdf:application/pdf},
}
